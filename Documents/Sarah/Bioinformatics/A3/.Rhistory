library(klaR)
library(e1071)
library(ROCR)
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {ifelse(x=='D', 1, -1)})
sample <- sample.int(n=nrow(spam), size = floor(.8*nrow(P53)), replace = F)
train <- P53[sample,]
test <- P53[-sample,]
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {ifelse(x=='D', 1, -1)})
sample <- sample.int(n=nrow(P53), size = floor(.8*nrow(P53)), replace = F)
train <- P53[sample,]
test <- P53[-sample,]
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE)
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
table(predictions, test$CLASS)
actual_class <- test$CLASS == 1
score <- attr(predictions, "probabilities")[,c(1)]
pred <- prediction(score, actual_class)
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
sample <- sample.int(n=nrow(P53), size = floor(.8*nrow(P53)), replace = F)
train <- P53[sample,]
test <- P53[-sample,]
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
data(spam)
library(kernlab)
data(spam)
View(spam)
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], as.factor())
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
#P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {ifelse(x=='D', 1, -1)})
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], as.factor())
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
#P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {ifelse(x=='D', 1, -1)})
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
sample <- sample.int(n=nrow(P53), size = floor(.8*nrow(P53)), replace = F)
train <- P53[sample,]
test <- P53[-sample,]
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
View(P53)
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
#P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {ifelse(x=='D', 1, -1)})
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
sample <- sample.int(n=nrow(P53), size = floor(.8*nrow(P53)), replace = F)
train <- P53[sample,]
test <- P53[-sample,]
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
table(predictions, test$CLASS)
actual_class <- test$CLASS == 1
score <- attr(predictions, "probabilities")[,c(1)]
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
pred <- prediction(score, actual_class)
perf <- performance(pred, "tpr", "fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
plot(perf, colorize=TRUE)
legend(0.6, 0.3,c(c(paste('AUC is', auc)), "\n"),
border="white", cex=1.0, box.col="white")
library(klaR)
library(e1071)
library(ROCR)
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(P53, 3)
fold_labels <- list()
fold_pred <- list()
count = 1
all_counts = list(1,2,3)
for (partition in partitions)
{
test <- partition
train <- setdiff(P53, test)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
fold_predictions <- list()
count = 1
all_counts = list(1,2,3)
for (partition in partitions)
{
test <- partition
train <- setdiff(P53, test)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p02 3-fold cross-validation.pdf')
pred <- prediction(fold_predictions, fold_labels)
perf <- performance(pred,"tpr","fpr")
plot(perf,col="grey82",lty=3)
plot(perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
dev.off()
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
#P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(P53, 3)
fold_labels <- list()
fold_predictions <- list()
count = 1
all_counts = list(1,2,3)
for (partition in partitions)
{
test <- partition
train <- setdiff(P53, test)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p02 3-fold cross-validation.pdf')
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(P53, 3)
fold_labels <- list()
fold_predictions <- list()
count = 1
all_counts = list(1,2,3)
for (partition in partitions)
{
test <- partition
train <- setdiff(P53, test)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p02 3-fold cross-validation.pdf')
pred <- prediction(fold_predictions, fold_labels)
perf <- performance(pred,"tpr","fpr")
plot(perf,col="grey82",lty=3)
plot(perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
dev.off()
library(klaR)
library(e1071)
library(ROCR)
library(kernlab)
data(spam)
set.seed(3)
sample <- sample.int(n=nrow(spam), size = floor(.8*nrow(spam)), replace = F)
train <- spam[sample,]
test <- spam[-sample,]
trainSVM <- svm(type ~ ., data = train, kernel = "linear", probability=TRUE)
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
table(predictions, test$type)
actual_class <- test$type == 'spam'
score <- attr(predictions, "probabilities")[,c("spam")]
pred <- prediction(score, actual_class)
perf <- performance(pred, "tpr", "fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
pdf('p01 no cross-validation.pdf')
plot(perf, colorize=TRUE)
legend(0.6, 0.3,c(c(paste('AUC is', auc)), "\n"),
border="white", cex=1.0, box.col="white")
dev.off()
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(spam, 10)
fold_predictions <- list()
fold_labels <- list()
count = 1
all_counts = list(1,2,3,4,5,6,7,8,9,10)
for (partition in partitions)
{
test <- partition
train <- setdiff(spam, test)
trainSVM <- svm(type ~ ., data = train, kernel = "linear", probability=TRUE)
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$type == 'spam'
score <- attr(predictions, "probabilities")[,c("spam")]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p01 10-fold cross-validation.pdf')
pred <- prediction(fold_predictions, fold_labels)
perf <- performance(pred,"tpr","fpr")
plot(perf,col="grey82",lty=3)
plot(perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
dev.off()
auc <- performance(pred, "auc")
View(auc)
mean_auc <- mean(auc)
mean_auc <- mean(auc.y_values)
mean_auc <- mean(auc$y_values)
mean_auc <- mean(auc['y_values'])
mean_auc <- mean(auc[['y.values']])
auc <- unlist(slot(auc, "y.values"))
mean_auc <- mean(auc)
library(klaR)
library(e1071)
library(ROCR)
P53 <- read.table("p03_Kato_P53_mutants_200.txt", sep = "\t", header = TRUE)
P53 <- subset(P53, select = -c(MUT))
P53[,c("CLASS")] <- sapply(P53[,c("CLASS")], function(x) {as.factor(x)})
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(P53, 3)
fold_labels <- list()
fold_predictions <- list()
count = 1
all_counts = list(1,2,3)
for (partition in partitions)
{
test <- partition
train <- setdiff(P53, test)
trainSVM <- svm(CLASS ~ ., data = train, kernel = "linear", probability=TRUE, type = 'C')
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$CLASS == 'D'
score <- attr(predictions, "probabilities")[,c('D')]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p02 3-fold cross-validation.pdf')
pred <- prediction(fold_predictions, fold_labels)
perf <- performance(pred,"tpr","fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
mean_auc <- mean(auc)
plot(perf,col="grey82",lty=3)
plot(perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
legend(0.6, 0.3,c(c(paste('AUC is', mean_auc)), "\n"),
border="white", cex=1.0, box.col="white")
dev.off()
library(klaR)
library(e1071)
library(ROCR)
library(kernlab)
data(spam)
set.seed(3)
sample <- sample.int(n=nrow(spam), size = floor(.8*nrow(spam)), replace = F)
train <- spam[sample,]
test <- spam[-sample,]
trainSVM <- svm(type ~ ., data = train, kernel = "linear", probability=TRUE)
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
table(predictions, test$type)
actual_class <- test$type == 'spam'
score <- attr(predictions, "probabilities")[,c("spam")]
pred <- prediction(score, actual_class)
perf <- performance(pred, "tpr", "fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
pdf('p01 no cross-validation.pdf')
plot(perf, colorize=TRUE)
legend(0.6, 0.3,c(c(paste('AUC is', auc)), "\n"),
border="white", cex=1.0, box.col="white")
dev.off()
folds <- function(x,n) split(x, sort(rep(1:n, len=length(x))))
partitions <- folds(spam, 10)
fold_predictions <- list()
fold_labels <- list()
count = 1
all_counts = list(1,2,3,4,5,6,7,8,9,10)
for (partition in partitions)
{
test <- partition
train <- setdiff(spam, test)
trainSVM <- svm(type ~ ., data = train, kernel = "linear", probability=TRUE)
predictions <- predict(trainSVM, test, probability=TRUE, type = 'raw')
actual_class <- test$type == 'spam'
score <- attr(predictions, "probabilities")[,c("spam")]
score_numbers <- unname(score)
fold_predictions[count] <- list(score_numbers)
fold_labels[count] <- list(actual_class)
count <- count + 1
}
pdf('p01 10-fold cross-validation.pdf')
pred <- prediction(fold_predictions, fold_labels)
perf <- performance(pred,"tpr","fpr")
auc <- performance(pred, "auc")
auc <- unlist(slot(auc, "y.values"))
mean_auc <- mean(auc)
plot(perf,col="grey82",lty=3)
plot(perf, lwd=3,avg="vertical",spread.estimate="boxplot",add=T)
legend(0.6, 0.3,c(c(paste('AUC is', mean_auc)), "\n"),
border="white", cex=1.0, box.col="white")
dev.off()
# load iris data set
data(iris)
# subset of iris data frame - extract only species versicolor and setosa
# we will only focus on the sepal and petal lengths of the dataset
irissubdf <- iris[1:100, c(1, 3, 5)]
names(irissubdf) <- c("sepal", "petal", "species")
head(irissubdf)
# plot data - a picture is worth a 1000 words. Melt data => then ggplot
library(ggplot2)
ggplot(irissubdf, aes(x = sepal, y = petal)) +
geom_point(aes(colour=species, shape=species), size = 3) +
xlab("sepal length") +
ylab("petal length") +
ggtitle("Species vs sepal and petal lengths")
# add binary labels corresponding to species - Initialize all values to 1
# add setosa label of -1. The binary +1, -1 labels are in the fourth
# column. It is better to create two separate data frames: one containing
# the attributes while the other contains the class values.
irissubdf[, 4] <- 1
irissubdf[irissubdf[, 3] == "setosa", 4] <- -1
x <- irissubdf[, c(1, 2)]
y <- irissubdf[, 4]
# head and tail of data
head(x)
head(y)
# write function that takes in the data frame, learning rate - eta, and number of epochs - n.iter and updates the weight factor. At this stage, I am only conserned with the final weight and the number of epochs required for the weight to converge
perceptron <- function(x, y, eta, niter) {
# initialize weight vector
weight <- rep(0, dim(x)[2] + 1)
errors <- rep(0, niter)
# loop over number of epochs niter
for (jj in 1:niter) {
# loop through training data set
for (ii in 1:length(y)) {
# Predict binary label using Heaviside activation
# function
z <- sum(weight[2:length(weight)] *
as.numeric(x[ii, ])) + weight[1]
if(z < 0) {
ypred <- -1
} else {
ypred <- 1
}
# Change weight - the formula doesn't do anything
# if the predicted value is correct
weightdiff <- eta * (y[ii] - ypred) *
c(1, as.numeric(x[ii, ]))
weight <- weight + weightdiff
# Update error function
if ((y[ii] - ypred) != 0.0) {
errors[jj] <- errors[jj] + 1
}
}
}
# weight to decide between the two species
print(weight)
return(errors)
}
err <- perceptron(x, y, 1, 10)
plot(1:10, err, type="l", lwd=2, col="red", xlab="epoch #", ylab="errors")
title("Errors vs epoch - learning rate eta = 1")
x <- irissubdf[, c(1, 2)]
y <- irissubdf[, 4]
perceptron(x,y,0.01, 10)
library(ROCR)
nbmodel <- NaiveBayes(Species~., data=iristrain)
nbprediction <- predict(nbmodel, iristest[,-5], type= 'raw')
library(ROCR)
library(klaR)
library(e1071)
nbmodel <- NaiveBayes(Species~., data=iristrain)
library(ROCR)
data(iris)
nbmodel <- NaiveBayes(Species~., data=iristrain)
nbprediction <- predict(nbmodel, iristest[,-5], type= 'raw')
testidx <- which(1:length(iris[,1])%%5 == 0)
iristrain <- iris[-testidx,]
iristest <- iris[testidx,]
nbmodel <- NaiveBayes(Species~., data=iristrain)
nbprediction <- predict(nbmodel, iristest[,-5], type= 'raw')
score <- nbprediction$posterior[, c("virginica")]
actual_class <- iristest$Species == 'virginica'
pred <- prediction(score, actual_class)
nbperf <- performance(pred, "tpr", "fpr")
nbauc <- performance(pred, "auc")
nbauc <- unlist(slot(nbauc, "y.values"))
plot(nbperf, colorize=TRUE)
legend(0.6,0.3,c(c(paste('AUC is', nbauc)),"\n"),
border="white",cex=1.0, box.col = "white")
help("performance")
nberr <- peformance(pred, "err")
nberr <- peformance(pred, "err")
nberr <- performance(pred, "err")
View(nberr)
plot(nberr)
